{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3: Decision Tree, AdaBoost and Random Forest\n",
    "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset and test the performance with testing data\n",
    "\n",
    "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "The dataset is the Heart Disease Data Set from UCI Machine Learning Repository. It is a binary classifiation dataset, the label is stored in `target` column. **Please note that there exist categorical features which need to be [one-hot encoding](https://www.datacamp.com/community/tutorials/categorical-data) before fit into your model!**\n",
    "See follow links for more information\n",
    "https://archive.ics.uci.edu/ml/datasets/heart+Disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "df = pd.read_csv(file_url)\n",
    "\n",
    "# find noise\n",
    "# noise_list = []\n",
    "# for i in range(df.shape[0]):\n",
    "#     if df['thal'][i] == '1' or df['thal'][i] == '2':\n",
    "#         noise_list.append(i)\n",
    "# print ('noise index: ', noise_list)\n",
    "# df = df.drop(noise_list)\n",
    "\n",
    "# # one-hot encoding\n",
    "# df_ohe = df.copy()\n",
    "# df_ohe = pd.get_dummies(df_ohe, columns=['thal'], prefix = ['thal'])\n",
    "# df_ohe.head()\n",
    "\n",
    "# # change the order (move target to last)\n",
    "# df_ohe_target = df_ohe['target']\n",
    "# df_ohe = df_ohe.drop(['target'], axis = 1)\n",
    "# df_ohe = df_ohe.assign(target=df_ohe_target)\n",
    "# df_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.load('train_idx.npy')\n",
    "test_idx = np.load('test_idx.npy')\n",
    "# train_idx = np.setdiff1d(train_idx, noise_list)  # delete noise\n",
    "# test_idx = np.setdiff1d(test_idx, noise_list)    # delete noise\n",
    "\n",
    "train_df = df.iloc[train_idx]\n",
    "test_df = df.iloc[test_idx]\n",
    "\n",
    "# one-hot encoding\n",
    "train_df_ohe = train_df.copy()\n",
    "train_df_ohe = pd.get_dummies(train_df_ohe, columns=['thal'], prefix = ['thal'])\n",
    "test_df_ohe = test_df.copy()\n",
    "test_df_ohe = pd.get_dummies(test_df_ohe, columns=['thal'], prefix = ['thal'])\n",
    "\n",
    "# change the order (move target to last)\n",
    "train_df_ohe_target = train_df_ohe['target']\n",
    "train_df_ohe = train_df_ohe.drop(['target'], axis = 1)\n",
    "train_df_ohe = train_df_ohe.assign(target=train_df_ohe_target)\n",
    "test_df_ohe_target = test_df_ohe['target']\n",
    "test_df_ohe = test_df_ohe.drop(['target'], axis = 1)\n",
    "test_df_ohe = test_df_ohe.assign(target=test_df_ohe_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal_fixed</th>\n",
       "      <th>thal_normal</th>\n",
       "      <th>thal_reversible</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>192</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>170</td>\n",
       "      <td>225</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "136   54    1   2       192   283    0        2      195      0      0.0   \n",
       "232   58    0   4       170   225    1        2      146      1      2.8   \n",
       "233   56    1   2       130   221    0        2      163      0      0.0   \n",
       "184   46    1   4       120   249    0        2      144      0      0.8   \n",
       "84    55    0   2       135   250    0        2      161      0      1.4   \n",
       "\n",
       "     slope  ca  thal_fixed  thal_normal  thal_reversible  target  \n",
       "136      1   1           0            0                1       0  \n",
       "232      2   2           1            0                0       1  \n",
       "233      1   0           0            0                1       0  \n",
       "184      1   0           0            0                1       0  \n",
       "84       2   0           0            1                0       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_ohe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54.,  1.,  2., ...,  0.,  0.,  1.],\n",
       "       [58.,  0.,  4., ...,  1.,  0.,  0.],\n",
       "       [56.,  1.,  2., ...,  0.,  0.,  1.],\n",
       "       ...,\n",
       "       [64.,  1.,  1., ...,  0.,  0.,  1.],\n",
       "       [44.,  1.,  3., ...,  0.,  1.,  0.],\n",
       "       [57.,  1.,  3., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train_df_ohe.iloc[:,:-1].values\n",
    "y_train = train_df_ohe.iloc[:,-1:].values\n",
    "X_test = test_df_ohe.iloc[:,:-1].values\n",
    "y_test = test_df_ohe.iloc[:,-1:].values\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(sequence):\n",
    "    _, cnt = np.unique(sequence, return_counts=True)\n",
    "    prob = cnt / sequence.shape[0]\n",
    "    g = 1 - np.sum([p**2 for p in prob])\n",
    "    return g\n",
    "\n",
    "\n",
    "def entropy(sequence):\n",
    "    _, cnt = np.unique(sequence, return_counts=True)\n",
    "    prob = cnt / sequence.shape[0]\n",
    "    e = -1 * np.sum([p*np.log2(p) for p in prob])\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = class 1,\n",
    "# 2 = class 2\n",
    "data = np.array([1,2,1,1,1,1,2,2,1,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini of data is  0.4628099173553719\n"
     ]
    }
   ],
   "source": [
    "print(\"Gini of data is \", gini(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of data is  0.9456603046006401\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy of data is \", entropy(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the test data. You should implement two arguments for the Decision Tree algorithm\n",
    "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
    "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, criterion='gini', max_depth=None):\n",
    "        self.criterion = criterion  # Setting the evaluation function (gini / entropy)\n",
    "        if criterion == 'gini':\n",
    "            self.measure_func = gini\n",
    "        else:\n",
    "            self.measure_func = entropy\n",
    "        self.max_depth = max_depth  # Setting the max depth of the tree\n",
    "        self.root = None\n",
    "        self.total_fi = None\n",
    "        return None\n",
    "\n",
    "    class Node():\n",
    "        def __init__(self):\n",
    "            self.feature = None\n",
    "            self.thres = None\n",
    "            self.impurity = None\n",
    "            self.data_num = None\n",
    "            self.left = None\n",
    "            self.right = None\n",
    "            self.predict_class = None\n",
    "\n",
    "    def get_thres(self, data):\n",
    "        thres = None\n",
    "        feature = None\n",
    "        split = False\n",
    "        min_impurity = None\n",
    "        impurity_in = self.measure_func(data[:, -1].astype(np.int32))\n",
    "        info_gain = 0\n",
    "        (n, dim) = data.shape\n",
    "        dim -= 1\n",
    "        for i in range(dim):\n",
    "            data_sorted = np.asarray(sorted(data, key=lambda t: t[i]))\n",
    "            for j in range(1, n):\n",
    "                t = (data_sorted[j - 1, i] + data_sorted[j, i]) / 2\n",
    "                left_data = data_sorted[data_sorted[:, i] < t]\n",
    "                right_data = data_sorted[data_sorted[:, i] >= t]\n",
    "                left_impurity = self.measure_func(left_data[:, -1].astype(np.int32))\n",
    "                right_impurity = self.measure_func(right_data[:, -1].astype(np.int32))\n",
    "                impurity = left_data.shape[0] * left_impurity\n",
    "                impurity += right_data.shape[0] * right_impurity\n",
    "                impurity /= data_sorted.shape[0]\n",
    "                if impurity_in - impurity > info_gain:\n",
    "                    split = True\n",
    "                    min_impurity = impurity\n",
    "                    thres = t\n",
    "                    feature = i\n",
    "                    info_gain = impurity_in - impurity\n",
    "        return split, feature, thres, min_impurity\n",
    "\n",
    "    def build_tree(self, data, depth=None):\n",
    "        node = self.Node()\n",
    "        ## --- check the X element are the same ---\n",
    "        checkSame = []\n",
    "        for i in range(data.shape[0] - 1):\n",
    "            if (data[i][:-1] == data[i+1][:-1]).all():\n",
    "                checkSame.append(True)\n",
    "            else:\n",
    "                checkSame.append(False)\n",
    "        ## ---------------- end -------------------\n",
    "        if self.measure_func(data[:, -1].astype(np.int32)) == 0:  # check impurity = 0\n",
    "            node.predict_class = int(data[0, -1])\n",
    "        elif len(data) == 0:                                      # check no element in a leaf\n",
    "            node.predict_class = 0\n",
    "        elif depth == 0:                                          # check depth = 0\n",
    "            label, cnt = np.unique(data[:, -1].astype(np.int32), return_counts=True)\n",
    "            node.predict_class = label[np.argmax(cnt)]\n",
    "        elif all(checkSame) == True:                              # check the same X element\n",
    "            label, cnt = np.unique(data[:, -1].astype(np.int32), return_counts=True)\n",
    "            node.predict_class = label[np.argmax(cnt)]\n",
    "        else:\n",
    "            split, feature, thres, impurity = self.get_thres(data)\n",
    "            if split == True:\n",
    "                node.feature = feature\n",
    "                node.thres = thres\n",
    "                node.impurity = impurity\n",
    "                node.data_num = data.shape[0]\n",
    "                left_data = data[data[:, feature] < thres]\n",
    "                right_data = data[data[:, feature] >= thres]\n",
    "                if depth is None:\n",
    "                    node.left = self.build_tree(left_data)\n",
    "                    node.right = self.build_tree(right_data)\n",
    "                else:\n",
    "                    node.left = self.build_tree(left_data, depth-1)\n",
    "                    node.right = self.build_tree(right_data, depth-1)\n",
    "            else:\n",
    "                label, cnt = np.unique(data[:, -1].astype(np.int32), return_counts=True)\n",
    "                node.predict_class = label[np.argmax(cnt)]\n",
    "        return node\n",
    "\n",
    "    def train(self, X, y):\n",
    "        data = np.hstack((X, y))\n",
    "        self.root = self.build_tree(data, self.max_depth)\n",
    "\n",
    "    def traverse(self, node, X):\n",
    "        if node.predict_class is not None:\n",
    "            return node.predict_class\n",
    "        else:\n",
    "            if X[node.feature] < node.thres:\n",
    "                return self.traverse(node.left, X)\n",
    "            else:\n",
    "                return self.traverse(node.right, X)\n",
    "\n",
    "    def print_acc(self, acc):\n",
    "        print(f'criterion = {self.criterion}')\n",
    "        print(f'max depth = {self.max_depth}')\n",
    "        print(f'acc       = {acc}')\n",
    "        print('====================')\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        pred = np.zeros(X.shape[0]).astype(np.int32)\n",
    "        correct = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            pred[i] = self.traverse(self.root, X[i])\n",
    "            if y is not None:\n",
    "                if pred[i] == y[i, 0]:\n",
    "                    correct += 1\n",
    "        acc = correct / X.shape[0] if y is not None else None\n",
    "        self.print_acc(acc)\n",
    "        return pred, acc\n",
    "\n",
    "    def get_fi(self, node):\n",
    "        if node.left and node.left.impurity is not None:\n",
    "            self.get_fi(node.left)\n",
    "        if node.right and node.right.impurity is not None:\n",
    "            self.get_fi(node.right)\n",
    "        self.total_fi[node.feature] += 1\n",
    "\n",
    "    def feature_importance(self):\n",
    "        self.total_fi = np.zeros(len(feature_names))\n",
    "        self.get_fi(self.root)\n",
    "        return self.total_fi\n",
    "\n",
    "    def print_tree(self, node, ident):\n",
    "        if node.predict_class is not None:\n",
    "            print(f'{ident}Predict {node.predict_class}')\n",
    "        else:\n",
    "            print(f'{ident}{node.feature} >= {node.thres}')\n",
    "            print(f'{ident}--> True:')\n",
    "            self.print_tree(node.right, ident + '  ')\n",
    "            print(f'{ident}--> False:')\n",
    "            self.print_tree(node.left, ident + '  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1\n",
    "Using `criterion=gini`, showing the accuracy score of test data by `max_depth=3` and `max_depth=10`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion = gini\n",
      "max depth = 3\n",
      "acc       = 0.79\n",
      "====================\n",
      "criterion = gini\n",
      "max depth = 10\n",
      "acc       = 0.73\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_depth3.train(X_train, y_train)\n",
    "_, acc = clf_depth3.predict(X_test, y_test)\n",
    "\n",
    "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
    "clf_depth10.train(X_train, y_train)\n",
    "_, acc = clf_depth10.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2\n",
    "Using `max_depth=3`, showing the accuracy score of test data by `criterion=gini` and `criterion=entropy`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion = gini\n",
      "max depth = 3\n",
      "acc       = 0.79\n",
      "====================\n",
      "criterion = entropy\n",
      "max depth = 3\n",
      "acc       = 0.76\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
    "clf_gini.train(X_train, y_train)\n",
    "_, acc = clf_gini.predict(X_test, y_test)\n",
    "\n",
    "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
    "clf_entropy.train(X_train, y_train)\n",
    "_, acc = clf_entropy.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Your decisition tree scores should over **0.7**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
    "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
    "- Hint: You can use the recursive method to build the nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
    "\n",
    "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance\n",
    "\n",
    "![image](https://i2.wp.com/sefiks.com/wp-content/uploads/2020/04/c45-fi-results.jpg?w=481&ssl=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
       "       'exang', 'oldpeak', 'slope', 'ca', 'thal_fixed', 'thal_normal',\n",
       "       'thal_reversible'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = train_df_ohe.columns\n",
    "feature_names = feature_names.drop(['target'])\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fi = clf_depth10.feature_importance()\n",
    "\n",
    "x_pos = [i for i, _ in enumerate(feature_names)]\n",
    "plt.barh(x_pos, fi)\n",
    "plt.ylabel('feature names')\n",
    "plt.xlabel('feature importance')\n",
    "plt.xticks(np.arange(max(fi)+1))\n",
    "plt.yticks(x_pos, feature_names)\n",
    "plt.gca().grid(axis='x', which='major')\n",
    "plt.tight_layout()\n",
    "# plt.savefig('fi.png', dpi=300, transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
    "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Adaboost():\n",
    "    \"\"\"Boosting method that uses a number of weak classifiers in \n",
    "    ensemble to make a strong classifier. This implementation uses decision\n",
    "    stumps, which is a one level Decision Tree. \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clf: int\n",
    "        The number of weak classifiers that will be used. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=10):\n",
    "        self.n_clf = n_estimators\n",
    "        # Determines if sample shall be classified as -1 or 1 given threshold\n",
    "        self.polarity = 1\n",
    "        # The index of the feature used to make classification\n",
    "        self.feature_index = None\n",
    "        # The threshold value that the feature should be measured against\n",
    "        self.threshold = None\n",
    "        # Value indicative of the classifier's accuracy\n",
    "        self.alpha = None\n",
    "\n",
    "    def train(self, X, y):\n",
    "        for i in range(len(y)):\n",
    "            if y[i] == 0:\n",
    "                y[i] = -1\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Initialize weights to 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "        \n",
    "        self.clfs = []\n",
    "        # Iterate through classifiers\n",
    "        for _ in range(self.n_clf):\n",
    "            # Minimum error given for using a certain feature value threshold\n",
    "            # for predicting sample label\n",
    "            min_error = float('inf')\n",
    "            # Iterate throught every unique feature value and see what value\n",
    "            # makes the best threshold for predicting y\n",
    "            for feature_i in range(n_features):\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "                # Try every unique feature value as threshold\n",
    "                for threshold in unique_values:\n",
    "                    p = 1\n",
    "                    # Set all predictions to '1' initially\n",
    "                    prediction = np.ones(np.shape(y))\n",
    "                    # Label the samples whose values are below threshold as '-1'\n",
    "                    prediction[X[:, feature_i] < threshold] = -1\n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    y = y.flatten()\n",
    "                    prediction = prediction.flatten()\n",
    "                    error = sum(w[y != prediction])\n",
    "                    \n",
    "                    # If the error is over 50% we flip the polarity so that samples that\n",
    "                    # were classified as 0 are classified as 1, and vice versa\n",
    "                    # E.g error = 0.8 => (1 - error) = 0.2\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # If this threshold resulted in the smallest error we save the\n",
    "                    # configuration\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_index = feature_i\n",
    "                        min_error = error\n",
    "            # Calculate the alpha which is used to update the sample weights,\n",
    "            # Alpha is also an approximation of this classifier's proficiency\n",
    "            clf.alpha = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n",
    "            # Set all predictions to '1' initially\n",
    "            predictions = np.ones(np.shape(y))\n",
    "            # The indexes where the sample values are below threshold\n",
    "            negative_idx = (clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold)\n",
    "            # Label those as '-1'\n",
    "            predictions[negative_idx] = -1\n",
    "            # Calculate new weights \n",
    "            # Missclassified samples gets larger weights and correctly classified samples smaller\n",
    "            w *= np.exp(-clf.alpha * y * predictions)\n",
    "            # Normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # Save classifier\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        n_samples = np.shape(X)[0]\n",
    "        y_pred = np.zeros((n_samples, 1))\n",
    "        # For each classifier => label the samples\n",
    "        for clf in self.clfs:\n",
    "            # Set all predictions to '1' initially\n",
    "            predictions = np.ones(np.shape(y_pred))\n",
    "            # The indexes where the sample values are below threshold\n",
    "            negative_idx = (clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold)\n",
    "            # Label those as '-1'\n",
    "            predictions[negative_idx] = -1\n",
    "            # Add predictions weighted by the classifiers alpha\n",
    "            # (alpha indicative of classifier's proficiency)\n",
    "            y_pred += clf.alpha * predictions\n",
    "\n",
    "        # Return sign of prediction sum\n",
    "        y_pred = np.sign(y_pred).flatten()\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == -1:\n",
    "                y_pred[i] = 0\n",
    "        \n",
    "        correct = 0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y[i]==y_pred[i]:\n",
    "                correct += 1\n",
    "        acc = correct / len(y_pred)\n",
    "        return y_pred, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.1\n",
    "Show the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_Adaboost = Adaboost(n_estimators=10)\n",
    "clf_Adaboost.train(X_train, y_train)\n",
    "y_pred, acc = clf_Adaboost.predict(X_test, y_test)\n",
    "print(y_test.flatten())\n",
    "print(y_pred)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
    "\n",
    "1. **n_estimators**: The number of trees in the forest. \n",
    "2. **max_features**: The number of random select features to consider when looking for the best split\n",
    "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(sys.getrecursionlimit())\n",
    "# sys.setrecursionlimit(10000)\n",
    "\n",
    "import random\n",
    "random.seed(2)\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = int(np.round(max_features))\n",
    "        self.boostrap = boostrap\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.clfs = []\n",
    "        for i in range(self.n_estimators):\n",
    "            self.clfs.append(DecisionTree(self.criterion, self.max_depth))\n",
    "        self.random_vecs = []\n",
    "        return None\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        for i in range(self.n_estimators):\n",
    "            random_vec = random.sample(range(X.shape[1]), self.max_features)\n",
    "            self.random_vecs.append(random_vec)\n",
    "            if self.boostrap:\n",
    "                sample_num = int(np.round(X.shape[0]*2/3))\n",
    "                subset_idx = random.sample(range(X.shape[0]), sample_num)\n",
    "                self.clfs[i].train(X[subset_idx][:, random_vec], y[subset_idx])\n",
    "            else:\n",
    "                self.clfs[i].train(X[:, random_vec], y)\n",
    "            # print(f'{i+1} trees completed')\n",
    "\n",
    "    def print_acc(self, acc):\n",
    "        print(f'n estimators = {self.n_estimators}')\n",
    "        print(f'max features = {self.max_features}')\n",
    "        print(f'boostrap     = {self.boostrap}')\n",
    "        print(f'criterion    = {self.criterion}')\n",
    "        print(f'max depth    = {self.max_depth}')\n",
    "        print(f'acc          = {acc}')\n",
    "        print('====================')\n",
    "\n",
    "    def predict(self, X, y=None):\n",
    "        pred = np.zeros(X.shape[0]).astype(np.int32)\n",
    "        correct = 0\n",
    "        for i in range(X.shape[0]):\n",
    "            vote = []\n",
    "            for j in range(self.n_estimators):\n",
    "                vote.append(self.clfs[j].traverse(self.clfs[j].root, X[i, self.random_vecs[j]]))\n",
    "            label, cnt = np.unique(vote, return_counts=True)\n",
    "            pred[i] = label[np.argmax(cnt)]\n",
    "            if y is not None:\n",
    "                if pred[i] == y[i, 0]:\n",
    "                    correct += 1\n",
    "        acc = correct / X.shape[0] if y is not None else None\n",
    "        self.print_acc(acc)\n",
    "        return pred, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.1\n",
    "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of test data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n estimators = 10\n",
      "max features = 4\n",
      "boostrap     = True\n",
      "criterion    = gini\n",
      "max depth    = None\n",
      "acc          = 0.81\n",
      "====================\n",
      "n estimators = 100\n",
      "max features = 4\n",
      "boostrap     = True\n",
      "criterion    = gini\n",
      "max depth    = None\n",
      "acc          = 0.8\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(X_train.shape[1]), max_depth=None)\n",
    "clf_10tree.train(X_train, y_train)\n",
    "_, acc = clf_10tree.predict(X_test, y_test)\n",
    "    \n",
    "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(X_train.shape[1]), max_depth=None)\n",
    "clf_100tree.train(X_train, y_train)\n",
    "_, acc = clf_100tree.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5.2\n",
    "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of test data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n estimators = 10\n",
      "max features = 4\n",
      "boostrap     = True\n",
      "criterion    = gini\n",
      "max depth    = None\n",
      "acc          = 0.8\n",
      "====================\n",
      "n estimators = 10\n",
      "max features = 15\n",
      "boostrap     = True\n",
      "criterion    = gini\n",
      "max depth    = None\n",
      "acc          = 0.77\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(X_train.shape[1]), max_depth=None)\n",
    "clf_random_features.train(X_train, y_train)\n",
    "_, acc = clf_random_features.predict(X_test, y_test)\n",
    "\n",
    "clf_all_features = RandomForest(n_estimators=10, max_features=X_train.shape[1], max_depth=None)\n",
    "clf_all_features.train(X_train, y_train)\n",
    "_, acc = clf_all_features.predict(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6.\n",
    "Try you best to get highest test accuracy score by \n",
    "- Feature engineering\n",
    "- Hyperparameter tuning\n",
    "- Implement any other ensemble methods, such as gradient boosting. Please note that you cannot call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = your_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary\n",
    "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
